{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages & install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/arunjangir245/airline-flight-dataset-schedule-performance-etc/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  carrier_code  flight_number origin_airport destination_airport        date  \\\n",
      "0           AS            121            SEA                 ANC  2019-05-01   \n",
      "1           F9            402            LAX                 DEN  2019-05-01   \n",
      "2           F9            662            SFO                 DEN  2019-05-01   \n",
      "3           F9            790            PDX                 DEN  2019-05-01   \n",
      "4           AS            108            ANC                 SEA  2019-05-01   \n",
      "\n",
      "   scheduled_elapsed_time tail_number  departure_delay  arrival_delay  \\\n",
      "0                     215      N615AS               -8            -16   \n",
      "1                     147      N701FR               17             -4   \n",
      "2                     158      N346FR               44             27   \n",
      "3                     156      N332FR               24             10   \n",
      "4                     210      N548AS               -9            -31   \n",
      "\n",
      "   delay_carrier  ...  HourlyPrecipitation_x  HourlyStationPressure_x  \\\n",
      "0              0  ...                    0.0                    29.59   \n",
      "1              0  ...                    0.0                    29.65   \n",
      "2              0  ...                    0.0                    29.98   \n",
      "3              0  ...                    0.0                    29.98   \n",
      "4              0  ...                    0.0                    30.18   \n",
      "\n",
      "   HourlyVisibility_x  HourlyWindSpeed_x     STATION_y  \\\n",
      "0                10.0                8.0  7.027253e+10   \n",
      "1                10.0                3.0  7.256500e+10   \n",
      "2                10.0                6.0  7.256500e+10   \n",
      "3                10.0                0.0  7.256500e+10   \n",
      "4                10.0                5.0  7.279302e+10   \n",
      "\n",
      "   HourlyDryBulbTemperature_y  HourlyPrecipitation_y  HourlyStationPressure_y  \\\n",
      "0                        42.0                    0.0                    30.16   \n",
      "1                        34.0                    0.0                    24.43   \n",
      "2                        34.0                    0.0                    24.43   \n",
      "3                        34.0                    0.0                    24.43   \n",
      "4                        44.0                    0.0                    29.58   \n",
      "\n",
      "   HourlyVisibility_y HourlyWindSpeed_y  \n",
      "0                10.0               3.0  \n",
      "1                 4.0               0.0  \n",
      "2                 4.0               0.0  \n",
      "3                 4.0               0.0  \n",
      "4                10.0               7.0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "weather_data_path = os.path.join(current_directory, \"archive\", \"05-2019.csv\")\n",
    "data  = pd.read_csv(weather_data_path)  \n",
    "print(data .head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HW3 PCA Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrier_code\tflight_number\torigin_airport\tdestination_airport\tdate\t\n",
    "# scheduled_elapsed_time\ttail_number\tdeparture_delay\tarrival_delay\t\n",
    "# delay_carrier\tdelay_weather\tdelay_national_aviation_system\tdelay_security\t\n",
    "# delay_late_aircarft_arrival\tcancelled_code\tyear\tmonth\tday\tweekday\t\n",
    "# scheduled_departure_dt\tscheduled_arrival_dt\tactual_departure_dt\tactual_arrival_dt\t\n",
    "# STATION_x\tHourlyDryBulbTemperature_x\tHourlyPrecipitation_x\tHourlyStationPressure_x\t\n",
    "# HourlyVisibility_x\tHourlyWindSpeed_x\tSTATION_y\tHourlyDryBulbTemperature_y\t\n",
    "# HourlyPrecipitation_y\tHourlyStationPressure_y\tHourlyVisibility_y\tHourlyWindSpeed_y\n",
    "\n",
    "# Replace \"N\" with 0 and \"B\" with 1 in the cancelled_code column\n",
    "data['cancelled_code'] = np.where(data['cancelled_code'] == 'N', 0, 1)\n",
    "\n",
    "###### FLIGHT\n",
    "flight_columns = [\"flight_number\", \"scheduled_elapsed_time\", \"cancelled_code\"]\n",
    "flight_data = data[flight_columns]\n",
    "flight_data_cleaned = flight_data.dropna()\n",
    "\n",
    "###### WEATHER\n",
    "weather_columns = [\n",
    "    \"HourlyDryBulbTemperature_x\", \n",
    "    \"HourlyPrecipitation_x\", \"HourlyStationPressure_x\", \"HourlyVisibility_x\", \n",
    "    \"HourlyWindSpeed_x\", \"HourlyDryBulbTemperature_y\", \n",
    "    \"HourlyPrecipitation_y\", \"HourlyStationPressure_y\", \n",
    "    \"HourlyVisibility_y\", \"HourlyWindSpeed_y\"\n",
    "]\n",
    "weather_data = data[weather_columns]\n",
    "weather_data_cleaned = weather_data.dropna()\n",
    "\n",
    "# For the arrival_delay, convert to numeric and handle non-numeric values\n",
    "arrival_delay = pd.to_numeric(data['arrival_delay'], errors='coerce')\n",
    "# Drop NaN values\n",
    "arrival_delay = arrival_delay.dropna()\n",
    "# Convert to numpy array for scaling\n",
    "y = arrival_delay.values\n",
    "\n",
    "# Now scale with MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "y_normalized = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# For the other scaling operations, ensure numeric data only\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "flight_scaled_data = scaler_std.fit_transform(flight_data_cleaned)\n",
    "# For weather data, ensure all columns are numeric\n",
    "numeric_weather_cols = weather_data_cleaned.select_dtypes(include=['number']).columns\n",
    "weather_scaled_data = scaler_std.fit_transform(weather_data_cleaned[numeric_weather_cols])                                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flight Reduced Data Shape: (694336, 3)\n"
     ]
    }
   ],
   "source": [
    "from weather_pca import PCA\n",
    "###### FLIGHT\n",
    "# Initialize PCA\n",
    "flight_pca = PCA()\n",
    "\n",
    "# Fit PCA on the scaled data\n",
    "flight_pca.fit(flight_scaled_data)\n",
    "\n",
    "# Transform data to reduce dimensions (e.g., retain 2 principal components)\n",
    "flight_reduced_data = flight_pca.transform(flight_scaled_data, K=3)\n",
    "\n",
    "print(\"Flight Reduced Data Shape:\", flight_reduced_data.shape)\n",
    "\n",
    "###### WEATHER\n",
    "# Initialize PCA\n",
    "weather_pca = PCA()\n",
    "\n",
    "# Fit PCA on the scaled data\n",
    "weather_pca.fit(weather_scaled_data)\n",
    "\n",
    "# Transform data to reduce dimensions (e.g., retain 2 principal components)\n",
    "weather_reduced_data = weather_pca.transform(weather_scaled_data, K=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [691747, 694336]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Split into train and test sets\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweather_reduced_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize PCA\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:2782\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2786\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2787\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [691747, 694336]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    weather_reduced_data, y_normalized, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "X_train_np = X_train.astype(float)\n",
    "X_test_np = X_test.astype(float)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit PCA on the scaled data\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled, K=3)\n",
    "X_test_pca = pca.transform(X_test_scaled, K=3)\n",
    "\n",
    "print(f\"X train pca shape: {X_train_pca.shape}\")\n",
    "print(f\"X test pca shape: {X_test_pca.shape}\")\n",
    "print(f\"Y train shape: {y_train.shape}\")\n",
    "\n",
    "# Randomly choose indices\n",
    "sample_size = min(5000, len(y_train))  # Ensure sample size does not exceed available data\n",
    "indices = np.random.choice(len(y_train), size=sample_size, replace=False)\n",
    "\n",
    "# Subset the data\n",
    "X_sample = X_train_pca[indices]\n",
    "y_sample = y_train[indices]  # Use direct indexing for NumPy arrays\n",
    "\n",
    "print(\"Sampled X shape:\", X_sample.shape)\n",
    "print(\"Sampled y shape:\", y_sample.shape)\n",
    "\n",
    "pca.visualize(X=X_sample, y=y_sample, fig_title=\"Weather PCA Projection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = pd.DataFrame(weather_reduced_data, columns=['PC1', 'PC2', 'PC3'])\n",
    "reduced_df.to_csv('reduced_05_2019.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
